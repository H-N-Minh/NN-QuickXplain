## Full Report on the `learn_linux_diagnosis` Codebase

This codebase is designed to train a Neural Network (NN) model to predict conflict sets in software product line configurations, specifically for a system named "arcade-game" (as indicated by `arcade-game.splx`). It then evaluates the model's performance in terms of accuracy and its ability to speed up the conflict diagnosis process performed by an external tool, QuickXplain (invoked via `fm_conflict.jar`).

Here's a chronological breakdown of the process:

**Overall Goal:** To use a machine learning model to predict which constraints in a given configuration are likely part of a conflict, and then use this prediction to order the constraints before feeding them to the QuickXplain solver. The hypothesis is that providing QuickXplain with a "smarter" order of constraints (those most likely to be in a conflict first) can lead to faster conflict detection and potentially smaller conflict sets.

**File Structure Overview:**

* **Root Folder:**
    * `learn_linux_diagnosis.py`: The main script orchestrating the entire process.
    * `data_handling.py`: Contains functions for reading and initially processing the training data from CSV files.
    * `data_preprocessing.py`: Contains functions for splitting the data into training, validation, and test sets.
    * `model_evaluation.py`: Defines the Neural Network model (class `ConLearn`), its training, evaluation, and the logic for interacting with the QuickXplain solver.
* **`TrainingData/arcade/` Folder:**
    * `invalid_confs_48752.csv`: Contains 48752 samples of invalid configurations. Each row is a sample, and each of the 47 columns (after dropping the index) represents a constraint. A value of `1` means the constraint is selected ("true"), and `-1` means it's not selected ("false").
    * `conflicts_48752.csv`: Contains the corresponding actual minimal conflict sets for the 48752 invalid configurations, as determined by QuickXplain beforehand. Each row corresponds to a sample in `invalid_confs_48752.csv`. A value of `1` means the constraint is part of a minimal conflict set for that sample, and `0` means it's not.
* **`Solver/` Folder:**
    * `arcade-game.splx`: The feature model definition for the "arcade-game" system, outlining the features and their constraints. This is used by QuickXplain.
    * `fm_conflict.jar`: The QuickXplain solver, a Java archive used to find minimal conflict sets.
    * `diagnosis_choco.py`: A Python script that acts as a wrapper to execute `fm_conflict.jar` with specified configurations and retrieve the results.
    * **`Solver/Input/` Folder (created during runtime):** Stores temporary text files (e.g., `conf0.txt`), each representing a single configuration to be analyzed by QuickXplain.
    * **`Solver/Output/` Folder (created during runtime):** Stores the output text files from QuickXplain (e.g., `conf0_output.txt`), detailing the identified conflict sets, runtime, and other metrics for each input configuration.
    * **`Solver/Logs/` Folder (created during runtime):** Stores log files generated by `fm_conflict.jar`.
* **`Models/` Folder (created during runtime):** Stores the trained Neural Network models and their training history plots.

---

### Step-by-Step Chronological Execution (based on `learn_linux_diagnosis.py`):

The execution starts within the `if __name__ == "__main__":` block of `learn_linux_diagnosis.py`, which calls the `learn_diagnosis` function with a `settings_dict`.

#### 1. Initialization and Timing

* `overall_start_time = time.time()`: Records the start time for the entire process.
* The `settings_dict` provides paths to various files and directories:
    * `CONSTRAINTS_FILE_PATH`: `TrainingData/arcade/invalid_confs_48752.csv`
    * `CONFLICT_FILE_PATH`: `TrainingData/arcade/conflicts_48752.csv`
    * Other paths for configuration files, diagnosis output, and model storage.

#### 2. Data Extraction

* `print("Extracting data from csv files...")`
* This step is handled by `data_handling.read_data(constraints_file, conflict_file)`:
    * **Inputs:**
        * `constraints_file`: Path to `invalid_confs_48752.csv`.
        * `conflict_file`: Path to `conflicts_48752.csv`.
    * **Process:**
        1.  Reads `invalid_confs_48752.csv` into a pandas DataFrame called `features_dataframe`. Each row represents a sample configuration, and columns represent constraints.
        2.  Reads `conflicts_48752.csv` into a pandas DataFrame called `labels_dataframe`. Each row represents the known conflict set for the corresponding configuration in `features_dataframe`.
        3.  **Drops the first column (index column)** from both DataFrames as it's not needed for training. So, 48 columns become 47 feature/label columns.
        4.  In `features_dataframe`: Replaces all occurrences of `-1` (representing "false" or constraint not selected) with `0`. This converts the input features to a binary format (`0` or `1`).
        5.  In `labels_dataframe`: Replaces all occurrences of `-1` with `1`.
        6.  Renames columns in `features_dataframe` to `feature_0, feature_1, ..., feature_46`.
        7.  Renames columns in `labels_dataframe` to `label_0, label_1, ..., label_46`.
    * **Outputs:** `features_dataframe` (48752 rows, 47 columns) and `labels_dataframe` (48752 rows, 47 columns).
* The time taken for this step is recorded as `data_time`.
* The output indicates this took `0.50 seconds`.

#### 3. Data Preprocessing for Learning

* `print("preparing data for learning...")`
* This step is handled by `data_preprocessing.data_preprocessing_learning(features_dataframe, labels_dataframe)`:
    * **Inputs:** `features_dataframe` and `labels_dataframe` from the previous step.
    * **Process:**
        1.  Converts the pandas DataFrames (`features_dataframe` and `labels_dataframe`) into NumPy arrays (`features` and `labels`). The `features` array will have dimensions (48752, 47) and `labels` array will also be (48752, 47).
        2.  **Data Splitting:** The data is split into training, validation, and test sets using `sklearn.model_selection.train_test_split` with `random_state=42` for reproducibility.
            * **First Split:**
                * 70% of the data becomes the **training set** (`train_x`, `train_labels`).
                * 30% of the data becomes a **temporary set** (`temp_x`, `temp_labels`).
                * Based on 48752 samples:
                    * Training: $48752 * 0.7 \approx 34126$ samples.
                    * Temporary: $48752 * 0.3 \approx 14625$ samples.
            * **Second Split (from the temporary set):**
                * The temporary set (30% of total) is further split. `test_size=1/3` means 1/3 of the temporary set becomes the test set, and 2/3 becomes the validation set.
                * **Validation set** (`val_x`, `val_labels`): 2/3 of `temp_x` (which is $14625 * (2/3) \approx 9750$ samples, or 20% of the total original data).
                * **Test set** (`test_x`, `test_labels`): 1/3 of `temp_x` (which is $14625 * (1/3) \approx 4875$ samples, or 10% of the total original data).
    * **Outputs:** `train_x`, `val_x` (used as `validate_x` in `learn_linux_diagnosis.py`), `train_labels`, `val_labels` (used as `validate_labels`), `test_x`, `test_labels`.
* The time taken for this step is recorded as `preprocess_time`.
* The output indicates this took `0.10 seconds`.

#### 4. Neural Network Model Training

* `print("Start training...")`
* This step is handled by `ConLearn.train_and_evaluate(train_x, validate_x, train_labels, validate_labels)` located in `model_evaluation.py`.
    * **Inputs:**
        * `train_x`: Training features (approx. 287 samples, 47 features each).
        * `validate_x`: Validation features (approx. 9750 samples, 47 features each).
        * `train_labels`: Training labels (approx. 287 samples, 47 labels each).
        * `validate_labels`: Validation labels (approx. 9750 samples, 47 labels each).
    * **Process:**
        1.  `input_shape = train_x.shape[1]`: This will be 47 (number of constraints/features).
        2.  `output_shape = train_labels.shape[1]`: This will also be 47 (as the model predicts the probability for each constraint to be in a conflict).
        3.  `print("train_and_evaluate::creating model...")`
        4.  **Model Creation (`ConLearn.create_model`):**
            * A `Sequential` model from `tensorflow.keras.models` is defined.
            * **Input Layer:** `Input(shape=(input_shape,))` - an input layer expecting `input_shape` (47) features.
            * **Hidden Layer 1:** `Dense(input_shape, activation='relu', kernel_initializer=HeNormal())`.
                * `Dense` means it's a fully connected layer.
                * `input_shape` (47) neurons.
                * `activation='relu'` (Rectified Linear Unit) activation function.
                * `kernel_initializer=HeNormal()`: He Normal initialization, suitable for ReLU activations, helps with training deep networks.
            * **Hidden Layer 2:** `Dense(input_shape, activation='relu', kernel_initializer=HeNormal())`.
                * Identical to the first hidden layer: 47 neurons, ReLU activation, He Normal initialization.
            * **Output Layer:** `Dense(output_shape, activation='sigmoid')`.
                * `output_shape` (47) neurons, one for each constraint.
                * `activation='sigmoid'`: Sigmoid activation function, which outputs a probability between 0 and 1 for each neuron. This is appropriate for binary classification (each constraint is either in conflict or not) and also for predicting the likelihood.
            * The model has 47 input features, two hidden layers each with 47 neurons, and an output layer with 47 neurons.
        5.  `print("train_and_evaluate:: Done creating model")`
        6.  `print("train_and_evaluate::compiling model and train it...")`
        7.  **Model Compilation:**
            * `optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005)`: Adam optimizer with a learning rate of 0.0005.
            * `loss="binary_crossentropy"`: Binary cross-entropy loss function, suitable for binary classification tasks (predicting 0 or 1 for each of the 47 output neurons) and when the output neurons have sigmoid activation.
            * `metrics=['accuracy']`: The model will track accuracy during training. Note from the output that the accuracy reported here is likely a form of categorical accuracy across all output neurons, not the "Hamming Score" reported later, which is an element-wise accuracy.
        8.  **Model Training (`model.fit`):**
            * The model is trained using `train_x` and `train_labels`.
            * `epochs=12`: The training process iterates through the entire training dataset 12 times.
            * `batch_size=1024`: The training data is processed in batches of 1024 samples. Since there are only ~287 training samples, each epoch will likely consist of only one batch ($287 < 1024$). This means all 287 samples are used to update the model weights in each step of an epoch.
            * `validation_data=(test_x, test_labels)`: The `validate_x` and `validate_labels` (passed as `test_x` and `test_labels` to `fit` method here, which is a common naming convention for validation data in Keras `fit`) are used to evaluate the model's performance on unseen data after each epoch. This helps monitor for overfitting.
            * `verbose=1`: Prints progress for each epoch. The output shows:
                * `Epoch X/12`
                * `34/34`: This indicates the number of batches.
                * `accuracy`: Training accuracy for the epoch.
                * `loss`: Training loss (binary cross-entropy) for the epoch.
                * `val_accuracy`: Validation accuracy for the epoch.
                * `val_loss`: Validation loss for the epoch.
                * The values for these metrics are printed for each of the 12 epochs.
        9.  `print("train_and_evaluate:: Done training model")`
        10. **Model Saving:**
            * `model_id = str(uuid.uuid4())`: A unique ID is generated for the model.
            * `model_dir = f'Models/{model_id}'`: A directory is created (e.g., `Models/some-unique-id/`).
            * `model.save(f'{model_dir}/model.keras')`: The trained Keras model is saved to this directory.
        11. **Saving Training History Plots:**
            * Plots for 'Model Loss' (training vs. validation) and 'Model Accuracy' (training vs. validation) over epochs are generated using `matplotlib.pyplot` and saved as `losses.png` and `accuracy.png` in the model's directory.
    * **Outputs:** `model_id` (a string) and `history` (a dictionary containing training/validation loss and accuracy for each epoch).
* The time taken for this step is recorded as `training_time`.
* The output indicates this took `9.16 seconds`.

#### 5. Validating Neural Network Model (Performance Evaluation)

* `print("Validating neural network model...")`
* This step is handled by `ConLearn.model_predict_conflict(id, test_x, test_labels)` from `model_evaluation.py`.
    * **Inputs:**
        * `id`: The `model_id` of the trained model.
        * `test_x`: Test features (approx. 4875 samples, 47 features each).
        * `test_labels`: True test labels (approx. 4875 samples, 47 labels each).
    * **Process:**
        1.  **Load Model:** `model = tf.keras.models.load_model(f'Models/{model_id}/model.keras')`. The previously trained and saved model is loaded.
        2.  **Predict Probabilities:** `predictions = model.predict(features_dataframe)`
            * *Note:* The code uses `features_dataframe` which is `test_x` from `learn_linux_diagnosis.py`. So, predictions are made on the test set.
            * The `predictions` variable will be a NumPy array of shape (number of test samples, 47), containing the predicted probabilities (from the sigmoid output layer) for each of the 47 constraints being part of a conflict.
        3.  **Optimal Threshold Calculation:**
            * `precisions, recalls, thresholds = precision_recall_curve(labels_dataframe.ravel(), predictions.ravel())`: Calculates precision and recall for various probability thresholds. `ravel()` flattens the arrays, treating all constraint predictions/labels as a single list.
            * `f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-10)`: Calculates F1 scores from precision and recall.
            * `optimal_idx = np.argmax(f1_scores)`: Finds the index of the highest F1 score.
            * `optimal_threshold = thresholds[optimal_idx]`: The threshold that yields the best F1 score is chosen as the `optimal_threshold`.
            * `print(f"Optimal threshold: {optimal_threshold:.4f}")` (e.g., `Optimal threshold: 0.1235`).
        4.  **Binary Predictions:** `binary_predictions = (predictions > optimal_threshold).astype(int)`. The predicted probabilities are converted to binary (0 or 1) predictions using the `optimal_threshold`.
        5.  **Core Metrics Calculation:**
            * `hamming_score = np.mean(labels_dataframe == binary_predictions)`: Element-wise accuracy. It's the proportion of constraint predictions that are correct (true positives + true negatives) / (total constraints).
            * `precision = precision_score(labels_dataframe, binary_predictions, average='weighted', zero_division=0)`
            * `recall = recall_score(labels_dataframe, binary_predictions, average='weighted', zero_division=0)`
            * `f1 = f1_score(labels_dataframe, binary_predictions, average='weighted', zero_division=0)`
            * `loss = tf.keras.losses.BinaryCrossentropy()(labels_dataframe, predictions).numpy()`: The binary cross-entropy loss on the test set using the raw probabilities.
            * `auc (ROC-AUC)`: Calculated by averaging ROC-AUC scores for each constraint column, weighted by the number of non-zero labels in that column.
            * `mcc (Matthews Correlation Coefficient)`: Averaged across constraints.
            * `auprc (Area Under Precision-Recall Curve)`: Weighted average.
            * `precision_at_k (K=5)`: For each sample, it checks the top 5 constraints with the highest predicted probabilities and calculates how many of them are actual positives. This is then averaged over all samples.
        6.  **Performance Comparison (NN-guided vs. Normal QuickXplain):**
            This is the core of evaluating the practical benefit of the NN model. The idea is to see if using the NN's predictions to order constraints for QuickXplain results in faster diagnosis.

            * **a. `ConLearn.get_NN_performance(features_dataframe, predictions)`:**
                * **Inputs:** `features_dataframe` (which is `test_x`) and `predictions` (the NN's probability outputs for `test_x`).
                * **Prepare Input for QuickXplain (Ordered by NN):**
                    1.  `input_constraints_dict`: A list of dictionaries is created. Each dictionary corresponds to a sample in `test_x`. The keys are constraint names (from `ARCARD_FEATURE_MODEL`), and values are `1` (true) or `-1` (false, though the `features_dataframe` was converted to 0/1, this part seems to expect the original mapping or implicitly handles it by creating "true"/"false" strings).
                        * *Correction:* `input_constraints_dict` correctly maps feature names to their original binary values (1 or -1) for each row in `features_dataframe` (`test_x`). The code `input_constraints_dict[idx][constraint_name] == 1` later confirms this.
                    2.  `feature_order_dicts`: A list of dictionaries. Each dictionary maps the *predicted probability* from the NN to the *constraint name* for a given sample.
                    3.  `ordered_features_list`: For each sample, the constraints are sorted in **descending order of their predicted probability** by the NN. This list now contains, for each sample, a list of constraint names ordered by the NN's prediction of their likelihood of being in a conflict.
                    4.  `print("model_predict_conflict::creating configs")`
                    5.  The `Solver/Input/` directory is cleared and recreated.
                    6.  For each sample in `test_x`:
                        * A text file (e.g., `conf0.txt`, `conf1.txt`, ...) is created in `Solver/Input/`.
                        * Each line in the file is a constraint in the format `Constraint Name <true/false>`, e.g., `Velocity false`.
                        * **Crucially, the order of constraints written to this file is determined by `ordered_features_list` (i.e., sorted by NN's prediction).** The actual value (true/false) comes from `input_constraints_dict` (the original input configuration).
                    7.  `print(f"===> Done!! creating config took {config_time:.2f} seconds")` (e.g., `12.80 seconds`).
                * **Run QuickXplain (NN-Ordered):**
                    1.  `print("model_predict_conflict::getting diagnosis...")`
                    2.  `get_linux_diagnosis(os.path.join("Solver/Input"))` is called. This function is in `Solver/diagnosis_choco.py`.
                        * It constructs a command to run `fm_conflict.jar`.
                        * The command specifies the feature model (`Solver/arcade-game.splx`) and the path to the directory containing the configuration files (`Solver/Input/`). `fm_conflict.jar` will process each `.txt` file in this directory.
                        * Output from `fm_conflict.jar` (stdout, stderr) is suppressed (`DEVNULL`).
                        * Log files (`.log`, `.zip`, `.tmp`) generated by `fm_conflict.jar` are moved to `Solver/Logs/`.
                        * A `data` directory (if created by `fm_conflict.jar`) is moved to `Solver/Output/`. This `Solver/Output/` directory will contain files like `conf0_output.txt`, `conf1_output.txt`, etc., with the diagnosis results.
                    3.  `print(f"===> Done!! getting diagnosis took {diagnosis_time:.2f} seconds")` (e.g., `45.57 seconds`).
                * **Extract Metrics from QuickXplain Output (NN-Ordered):**
                    1.  `print("model_predict_conflict::extracting metrics...")`
                    2.  `avg_runtime, avg_cc = extract_metrics_optimized(data_folder)` is called, where `data_folder` is `Solver/Output/`.
                        * `extract_metrics_optimized` processes all `_output.txt` files in `Solver/Output/` in parallel using `ProcessPoolExecutor`.
                        * For each file (e.g., `conf0_output.txt`):
                            * It parses the file to extract the "Runtime" (in seconds) and "CC" (Conflict Count or Cardinality of Conflict, the number of constraints in the minimal conflict set).
                            * The `process_file` function handles reading these specific lines.
                        * It calculates the average runtime and average CC across all processed output files.
                    3.  `print(f"Average runtime for ordered: {avg_runtime:.6f} seconds")` (e.g., `0.002102 seconds`).
                    4.  `print(f"Average CC for ordered: {avg_cc:.2f}")` (e.g., `9.49`).
                    5.  `print(f"===> Done!! extracting metrics took {extract_time:.2f} seconds")` (e.g., `47.88 seconds`).
                * **Returns:** `avg_runtime` and `avg_cc` for the NN-ordered QuickXplain runs.

            * **b. `ConLearn.get_normal_performance(features_dataframe)`:**
                * **Inputs:** `features_dataframe` (which is `test_x`).
                * **Purpose:** To get baseline performance metrics from QuickXplain when it processes the configurations in their *original (default/arbitrary) order*, without guidance from the NN.
                * **Prepare Input for QuickXplain (Normal Order):**
                    1.  `print("model_predict_conflict::creating configs")`
                    2.  The `Solver/Input/` directory is cleared and recreated.
                    3.  For each sample in `test_x` (`features_dataframe`):
                        * A text file (e.g., `conf0.txt`) is created in `Solver/Input/`.
                        * Constraints are written to the file using their original order as defined by `ARCARD_FEATURE_MODEL`. The value (`true`/`false`) comes from the row in `features_dataframe`.
                    4.  `print(f"===> Done!! creating config took {config_time:.2f} seconds")` (e.g., `15.78 seconds`).
                * **Run QuickXplain (Normal Order):**
                    1.  `print("model_predict_conflict::getting diagnosis...")`
                    2.  `get_linux_diagnosis(os.path.join("Solver/Input"))` is called, similar to the NN-ordered run.
                    3.  `print(f"===> Done!! getting diagnosis took {diagnosis_time:.2f} seconds")` (e.g., `64.23 seconds`).
                * **Extract Metrics from QuickXplain Output (Normal Order):**
                    1.  `avg_runtime, avg_cc = extract_metrics_optimized(data_folder)` is called again.
                    2.  `print(f"Average runtime for normal: {avg_runtime:.6f} seconds")` (e.g., `0.003784 seconds`).
                    3.  `print(f"Average CC for normal: {avg_cc:.2f}")` (e.g., `13.11`).
                    4.  `print(f"===> Done!! extracting metrics took {extract_time:.2f} seconds")` (e.g., `99.49 seconds`).
                * **Returns:** `avg_runtime` and `avg_cc` for the normal QuickXplain runs.

            * **c. Calculate Performance Improvements:**
                * `runtime_improvement = normal_runtime - nn_runtime`
                * `cc_improvement = normal_cc - nn_cc`
                * `runtime_improvement_percentage = (runtime_improvement / nn_runtime) * 100`
                    * *Note:* The log shows "Faster %". If `nn_runtime` is smaller, this percentage shows how much faster the NN-guided approach is *relative to the NN-guided runtime*. A more standard way might be `(normal_runtime - nn_runtime) / normal_runtime * 100` to show reduction relative to the original. The current formula: $(normal - nn) / nn$. Example: normal=10s, nn=2s. $(10-2)/2 = 8/2 = 400\%$.
                * `cc_improvement_percentage = (cc_improvement / normal_cc) * 100`
                    * *Note:* The log shows "CC less %". This is calculated as `(normal_cc - nn_cc) / normal_cc * 100`. Example: normal_cc=10, nn_cc=7. $(10-7)/10 = 3/10 = 30\%$. This is a standard percentage reduction.

        7.  **Print Final Results:**
            * `print("\n-------FINAL RESULTS------")`
            * Prints all the calculated metrics: Hamming Score, Precision, Recall, F1 Score, MCC, AUPRC, Precision at K=5, ROC-AUC, Loss, Faster %, CC less %.
            * The example output shows these values (e.g., `Hamming Score: 0.9234`, `Faster %: 80.02%`, `CC less %: 27.60%`).

* The time taken for this entire validation step is recorded as `validation_time`.
* The output indicates this took `287.42 seconds`. This is the most time-consuming part, largely due to running QuickXplain multiple times.

#### 6. Execution Time Summary

* `overall_end_time = time.time()`: Records the end time for the entire process.
* `overall_time = overall_end_time - overall_start_time`: Calculates total execution time.
* `print("\n===== EXECUTION TIME SUMMARY =====")`
* Prints the time taken for each major step (Data Extraction, Data Preprocessing, Model Training, Model Validation) and their percentage of the total execution time.
    * Data Extraction: 0.50 seconds (0.2%)
    * Data Preprocessing: 0.10 seconds (0.0%)
    * Model Training: 9.16 seconds (3.1%)
    * Model Validation: 287.42 seconds (96.7%)
    * Total Execution: 297.17 seconds (100%)
* `print("=================================")`
* `print("==> Done Everything...")`

---

### Summary of Neural Network Model Details:

* **Type:** Feedforward Neural Network (Multilayer Perceptron).
* **Input Layer:**
    * Number of neurons: 47 (corresponding to the 47 constraints in `invalid_confs_48752.csv` after removing the index column).
* **Hidden Layers:**
    * Number of hidden layers: 2.
    * Neurons per hidden layer: 47 in the first, 47 in the second.
    * Activation function: ReLU (`relu`).
    * Kernel initializer: He Normal (`HeNormal`).
* **Output Layer:**
    * Number of neurons: 47 (one for each input constraint).
    * Activation function: Sigmoid (`sigmoid`), outputting a probability for each constraint.
* **Loss Function:** Binary Cross-Entropy (`binary_crossentropy`).
* **Optimizer:** Adam (`tf.keras.optimizers.Adam`) with a learning rate of `0.0005`.
* **Training Process:**
    * Epochs: 12.
    * Batch Size: 1024.
    * Data Split: 70% training, 20% validation, 10% test.
    * Metrics Monitored: `accuracy` (during training/validation epochs), and a comprehensive suite of metrics (Hamming, Precision, Recall, F1, MCC, AUPRC, ROC-AUC, Loss) during the final validation on the test set.
* **Purpose of the NN:** To predict the probability that each of the 47 input constraints is part of a minimal conflict set for a given invalid configuration. These probabilities are then used to order the constraints when preparing input for the QuickXplain diagnosis tool.

---

### QuickXplain Interaction:

* **Input Preparation for QuickXplain:**
    * For each configuration sample (from the test set `test_x`):
        * A text file (e.g., `conf0.txt`) is generated in the `Solver/Input/` directory.
        * Each line in this file represents a constraint in the format: `Constraint_Name constraint_value` (e.g., `Play Pong true`).
        * **NN-Guided Run:** The order of constraints in `conf<N>.txt` is determined by the NN's predicted probabilities (highest probability first).
        * **Normal Run:** The order of constraints in `conf<N>.txt` is the default order (likely from `ARCARD_FEATURE_MODEL`).
* **Execution of QuickXplain:**
    * The `diagnosis_choco.py` script calls `fm_conflict.jar` (the QuickXplain solver).
    * It passes the feature model (`arcade-game.splx`) and the directory containing the input configuration files (`Solver/Input/`).
    * QuickXplain processes each configuration file.
* **Output Processing from QuickXplain:**
    * QuickXplain generates output files (e.g., `conf0_output.txt`) in the `Solver/Output/` directory (after being moved from a `data` directory).
    * Each output file contains:
        * The original configuration.
        * An indication of whether it's `inconsistent`.
        * `CS: [...]`: The minimal conflict set found (e.g., `CS: [Velocity=false, Top Paddle=true]`).
        * `Runtime: ... seconds`: The time QuickXplain took for that specific configuration.
        * `CC: ...`: The cardinality of the conflict set (number of constraints in `CS`).
    * The `extract_metrics_optimized` function in `model_evaluation.py` parses these output files to extract the `Runtime` and `CC` values, which are then averaged over all test samples.

